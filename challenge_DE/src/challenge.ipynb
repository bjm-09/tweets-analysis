{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluciÃ³n y todas las suposiciones que estÃ¡s considerando. AquÃ­ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../data/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrote a simple set of tests using pytest so that we can be sure that the outputs match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.6, pytest-8.1.1, pluggy-1.4.0\n",
      "rootdir: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "tests.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 78.76s (0:01:18)\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outputs** -- With memory profiling and time tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For excercise one everything is the same, as it is explained in the q1_time script, for the rest of functions we can see in the output of the memory profiler and also with the time tracker, how the functions with a memory optimization approach take longer to run but use less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Time\n",
      "Filename: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     48.0 MiB     48.0 MiB           1   @profile\n",
      "     9                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                         \n",
      "    11                                             \"\"\"\n",
      "    12                                             I tried parallelizing this function as well so that it would be time optimized (like the following ones),\n",
      "    13                                             But I think I wasn't being able to do  it properly as running parallel functions and merging the results\n",
      "    14                                             was taking longer than just do it sequentially. Perhaps because of the double complexity of finding \n",
      "    15                                             the most common date and also retrieving the user with most tweets on it.\n",
      "    16                                             So I am finally leaving the solution that runs faster.\n",
      "    17                                             \"\"\"\n",
      "    18                                         \n",
      "    19                                             # Using a defaultdict we will have constant-time complexity when we insert values and perform lookups\n",
      "    20     48.0 MiB   -486.4 MiB          27       agg_dict = defaultdict(lambda: defaultdict(int))\n",
      "    21                                         \n",
      "    22                                             # try-catch in case the file doesn't exist or the path is wrong\n",
      "    23     48.0 MiB      0.0 MiB           1       try:\n",
      "    24                                                 # Open json file with tweets data to loop through every record\n",
      "    25     48.0 MiB    -30.5 MiB           2           with open(file_path, 'r') as file:\n",
      "    26     48.3 MiB -2612411.1 MiB      117408               for line in file:\n",
      "    27     48.3 MiB -2612383.3 MiB      117407                   tweet = json.loads(line)\n",
      "    28                                                         \n",
      "    29                                                         # We are interested only on dates, not datetimes so we create a key for every date\n",
      "    30                                                         # We also create a key with every date-key with the username, as we need to keep track of how many \n",
      "    31                                                         # tweets each user has on that specific day\n",
      "    32     48.3 MiB -2612385.7 MiB      117407                   try:\n",
      "    33     48.3 MiB -2612401.8 MiB      117407                       date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
      "    34     48.3 MiB -2612404.2 MiB      117407                       username = tweet['user']['username']\n",
      "    35     48.3 MiB -2612401.7 MiB      117407                       agg_dict[date][username] += 1\n",
      "    36                                         \n",
      "    37                                                         except ValueError as e:\n",
      "    38                                                             print(f\"Error parsing date for tweet: {e}\")\n",
      "    39                                                             continue  # Skip this record and continue to the next one\n",
      "    40                                         \n",
      "    41                                                 # Sort our dictionary by the total tweets, in descending order\n",
      "    42                                                 ## We sort the values only once, at the end\n",
      "    43     18.5 MiB    -29.5 MiB          27           top_10_dates = sorted(agg_dict.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]\n",
      "    44                                         \n",
      "    45                                                 # Previous step left us only with the 10 dates with most tweets\n",
      "    46                                                 ## Now we need to get the user with most tweets from each date to match the expected output\n",
      "    47     19.6 MiB      1.1 MiB          13           return [(date, max(users, key=users.get)) for date, users in top_10_dates]\n",
      "    48                                         \n",
      "    49                                             except FileNotFoundError as e:\n",
      "    50                                                 return(f\"File not found: {e}\")\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "The job finished running un 17.05 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1 Time\")\n",
    "!python3 q1_time.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Memory\n",
      "Filename: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     49.0 MiB     49.0 MiB           1   @profile\n",
      "     9                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                         \n",
      "    11                                             # Using a defaultdict we will have constant-time complexity when we insert values and perform lookups\n",
      "    12     49.7 MiB   -419.5 MiB          27       agg_dict = defaultdict(lambda: defaultdict(int))\n",
      "    13                                         \n",
      "    14                                             # try-catch in case the file doesn't exist or the path is wrong\n",
      "    15     49.0 MiB      0.0 MiB           1       try:\n",
      "    16                                                 # Open json file with tweets data to loop through every record\n",
      "    17     49.0 MiB    -31.7 MiB           2           with open(file_path, 'r') as file:\n",
      "    18                                         \n",
      "    19                                                     # By reading the file line by line we are avoiding unnecessary memory usage.\n",
      "    20     49.7 MiB -2317750.7 MiB      117408               for line in file:\n",
      "    21     49.7 MiB -2317721.7 MiB      117407                   tweet = json.loads(line)\n",
      "    22                                                         \n",
      "    23                                                         # We are interested only on dates, not datetimes so we create a key for every date\n",
      "    24                                                         # We also create a key with every date-key with the username, as we need to keep track of how many \n",
      "    25                                                         # tweets each user has on that specific day\n",
      "    26     49.7 MiB -2317724.3 MiB      117407                   try:\n",
      "    27     49.7 MiB -2317741.6 MiB      117407                       date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
      "    28     49.7 MiB -2317744.0 MiB      117407                       username = tweet['user']['username']\n",
      "    29     49.7 MiB -2317745.0 MiB      117407                       agg_dict[date][username] += 1\n",
      "    30                                         \n",
      "    31                                                         except ValueError as e:\n",
      "    32                                                             print(f\"Error parsing date for tweet: {e}\")\n",
      "    33                                                             continue  # Skip this record and continue to the next one\n",
      "    34                                                 \n",
      "    35                                                 # Sort our dictionary by the total tweets, in descending order\n",
      "    36                                                 ## We sort the values only once, at the end\n",
      "    37     18.8 MiB    -30.2 MiB          27           top_10_dates = sorted(agg_dict.items(), key=lambda x: sum(x[1].values()), reverse=True)[:10]\n",
      "    38                                         \n",
      "    39                                                 # Previous step left us only with the 10 dates with most tweets\n",
      "    40                                                 ## Now we need to get the user with most tweets from each date to match the expected output\n",
      "    41     19.8 MiB      1.0 MiB          13           return [(date, max(users, key=users.get)) for date, users in top_10_dates]\n",
      "    42                                         \n",
      "    43                                             except FileNotFoundError as e:\n",
      "    44                                                 return(f\"File not found: {e}\")\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n",
      "The job finished running un 17.16 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1 Memory\")\n",
    "!python3 q1_memory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 Time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    27     65.7 MiB     65.7 MiB           1   @profile\n",
      "    28                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    29                                         \n",
      "    30                                             # We will update this counter with the output of each pool so we can aggregate at the end\n",
      "    31     65.7 MiB      0.0 MiB           1       emojis_counter = Counter()\n",
      "    32                                         \n",
      "    33                                             # try-catch in case the file doesn't exist or the path is wrong\n",
      "    34     65.7 MiB      0.0 MiB           1       try:\n",
      "    35                                                 # Open json file with tweets data to loop through every record\n",
      "    36    186.9 MiB      0.0 MiB           2           with open(file_path, 'r') as file:\n",
      "    37                                         \n",
      "    38                                                     # Create a pool of processes to run in parallel\n",
      "    39    186.2 MiB      0.7 MiB           2               with Pool() as pool:\n",
      "    40                                         \n",
      "    41                                                         # Apply the get_emojis function with file path argument\n",
      "    42     66.3 MiB      0.0 MiB           1                   process_line = partial(get_emojis)\n",
      "    43                                         \n",
      "    44                                                         # Map the get_emojis function to each line in the file using the pool\n",
      "    45    186.2 MiB    119.8 MiB           1                   results = pool.map(process_line, file)\n",
      "    46                                         \n",
      "    47                                                     # Group the list of lists of emojis into a single list\n",
      "    48    186.8 MiB      0.6 MiB      163046               emojis = [emoji for sublist in results for emoji in sublist]\n",
      "    49                                         \n",
      "    50                                                     # Use the update method to feed the counter with the new data that we have collected\n",
      "    51    186.9 MiB      0.0 MiB           1               emojis_counter.update(emojis)\n",
      "    52                                         \n",
      "    53                                                 # Use the most_common method we retreive the top 10 most used emojis across all tweets\n",
      "    54    186.9 MiB      0.0 MiB           1           return emojis_counter.most_common(10)\n",
      "    55                                             \n",
      "    56                                             except FileNotFoundError as e:\n",
      "    57                                                 return(f\"File not found: {e}\")\n",
      "\n",
      "\n",
      "[('ðŸ™', 7286), ('ðŸ˜‚', 3072), ('ðŸšœ', 2972), ('âœŠ', 2411), ('ðŸŒ¾', 2363), ('ðŸ»', 2080), ('â¤', 1779), ('ðŸ¤£', 1668), ('ðŸ½', 1218), ('ðŸ‘‡', 1108)]\n",
      "The job finished running un 3.78 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q2 Time\")\n",
    "!python3 q2_time.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2 Memory\n",
      "Filename: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     64.9 MiB     64.9 MiB           1   @profile\n",
      "     9                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    10                                             \n",
      "    11                                             # Counter objects provide time optimized methods like the one we will be using to get the most popular emojis\n",
      "    12     64.9 MiB      0.0 MiB           1       emojis_counter = Counter()\n",
      "    13                                         \n",
      "    14                                             # try-catch in case the file doesn't exist or the path is wrong\n",
      "    15     64.9 MiB      0.0 MiB           1       try:\n",
      "    16                                                 # Open json file with tweets data to loop through every record\n",
      "    17     64.9 MiB    -47.4 MiB           2           with open(file_path, 'r') as file:\n",
      "    18                                                     # By reading the file line by line we are avoiding unnecessary memory usage.\n",
      "    19     65.1 MiB -5298443.3 MiB      117408               for line in file:\n",
      "    20     65.1 MiB -5298395.4 MiB      117407                   tweet = json.loads(line)\n",
      "    21                                         \n",
      "    22                                                         # try-catch in case the content key does not exist\n",
      "    23     65.1 MiB -5298395.6 MiB      117407                   try:\n",
      "    24     65.1 MiB -5298393.8 MiB      117407                       tweet = json.loads(line)\n",
      "    25     65.1 MiB -5298394.2 MiB      117407                       text = tweet['content']\n",
      "    26                                                             # We loop through the tweet text and append all emojis to a list\n",
      "    27                                                             # If it doesn't exist we move to the next tweet\n",
      "    28     65.1 MiB -784448415.5 MiB    17386304                       emojis = [c for c in text if c in EMOJI_DATA.keys()]\n",
      "    29                                         \n",
      "    30                                                         except KeyError:\n",
      "    31                                                             print(\"Tweet does not have 'content' key.\") \n",
      "    32                                                             continue\n",
      "    33                                         \n",
      "    34                                                         # Use the update method to feed the counter with the new data that we have collected\n",
      "    35     65.1 MiB -5298445.8 MiB      117407                   emojis_counter.update(emojis)\n",
      "    36                                         \n",
      "    37                                                 # Use the most_common method we retreive the top 10 most used emojis across all tweets\n",
      "    38     18.2 MiB    -46.7 MiB           1           return emojis_counter.most_common(10)\n",
      "    39                                         \n",
      "    40                                             except FileNotFoundError as e:\n",
      "    41                                                 return(f\"File not found: {e}\")\n",
      "\n",
      "\n",
      "[('ðŸ™', 7286), ('ðŸ˜‚', 3072), ('ðŸšœ', 2972), ('âœŠ', 2411), ('ðŸŒ¾', 2363), ('ðŸ»', 2080), ('â¤', 1779), ('ðŸ¤£', 1668), ('ðŸ½', 1218), ('ðŸ‘‡', 1108)]\n",
      "The job finished running un 174.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q2 Memory\") #This one takes forever to run so please be patient if you run this cell\n",
    "!python3 q2_memory.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Time\n",
      "Filename: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    42     48.3 MiB     48.3 MiB           1   @profile\n",
      "    43                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    44                                         \n",
      "    45    195.3 MiB      0.0 MiB           2       with open(file_path, 'r') as file:\n",
      "    46                                         \n",
      "    47                                                 # Create a pool of processes to run in parallel\n",
      "    48    195.3 MiB      0.7 MiB           2           with Pool() as pool:\n",
      "    49                                         \n",
      "    50                                                     # Apply the get_mentions function with file path argument\n",
      "    51     49.1 MiB      0.0 MiB           1               process_line = partial(get_mentions)\n",
      "    52                                         \n",
      "    53                                                     # Map the get_mentions function to each line in the file using the pool\n",
      "    54    195.4 MiB    146.2 MiB           1               counters = pool.map(process_line, file)\n",
      "    55                                         \n",
      "    56                                                 # Merge all counters together using the helper function\n",
      "    57    195.3 MiB      0.0 MiB           1           mentions_counter = merge_counters(counters)\n",
      "    58                                         \n",
      "    59                                             # Use the most_common method we retreive the top 10 most mentioned users across all tweets\n",
      "    60    195.3 MiB      0.0 MiB           1       return mentions_counter.most_common(10)\n",
      "\n",
      "\n",
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n",
      "The job finished running in 2.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q3 Time\")\n",
    "!python3 q3_time.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 Memory\n",
      "Filename: /Users/bjuanm/Desktop/Interviews/LATAM/tweets-analysis/challenge_DE/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     48.9 MiB     48.9 MiB           1   @profile\n",
      "     8                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     9                                             \n",
      "    10     48.9 MiB      0.0 MiB           1       mentions_counter = Counter()\n",
      "    11                                         \n",
      "    12                                             # I found out that there is a mentionedUsers key with the tweet mentions so I will use that instead\n",
      "    13                                             # of the regular expression that I used to have\n",
      "    14     50.9 MiB      0.0 MiB           2       with open(file_path, 'r') as file:\n",
      "    15     50.9 MiB      0.5 MiB      117408           for line in file:\n",
      "    16     50.9 MiB      1.1 MiB      117407               tweet = json.loads(line)\n",
      "    17                                         \n",
      "    18     50.9 MiB      0.1 MiB      117407               if tweet['mentionedUsers']:\n",
      "    19                                                         \n",
      "    20                                                         # Loop through all mentioned users in the tweet\n",
      "    21     50.9 MiB      0.0 MiB      141437                   for user in tweet['mentionedUsers']:\n",
      "    22                                         \n",
      "    23                                                             # Increase that usernames counter by 1\n",
      "    24     50.9 MiB      0.4 MiB      103403                       mentions_counter[user['username']] += 1\n",
      "    25                                         \n",
      "    26                                             # Use the most_common method we retreive the top 10 most mentioned users across all tweets\n",
      "    27     50.9 MiB      0.0 MiB           1       return mentions_counter.most_common(10)\n",
      "\n",
      "\n",
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n",
      "The job finished running un 8.5 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q3 Memory\")\n",
    "!python3 q3_memory.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
